{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda5ee77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\t [ 76733.33333333 291906.66666667  80556.66666667 123156.66666667]\n",
      "Labels:\t\t [72100.0, 279600.0, 82700.0, 112500.0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from six.moves import urllib\n",
    "from scipy.stats import expon, reciprocal\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CONSTANT DATA SOURCE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\")\n",
    "MODEL_PATH = os.path.join(\"models\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ TRANSFORMERS AND FUNCTIONS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "def load_housing_data():    \n",
    "    if not os.path.isdir(HOUSING_PATH):\n",
    "        os.makedirs(HOUSING_PATH)\n",
    "    tgz_path = os.path.join(HOUSING_PATH, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(HOUSING_URL, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(HOUSING_PATH)\n",
    "    housing_tgz.close()\n",
    "    csv_path = os.path.join(HOUSING_PATH, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "    \n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "def choose_top_functions(arr,k):\n",
    "    k = min(k,len(np.array(arr)))\n",
    "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "\n",
    "class RemoveUnnecessaryFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, feature_importance, k = 5 ):\n",
    "        self.feature_importance = feature_importance\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_ind = choose_top_functions(self.feature_importance,self.k)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[:,self.feature_ind]\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ DATA PREPARATION ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"], bins = [0.,1.5,3.0,4.5,6,np.inf],labels=[1,2,3,4,5])\n",
    "stratfold = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in stratfold.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set, housing):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)    \n",
    "    \n",
    "housing = strat_train_set.drop(\"median_house_value\",axis = 1)\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PIPELINES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "    \n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PROCESSING AND MODELLING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "svm_reg = joblib.load(MODEL_PATH+\"\\SVMRegressorBest.pkl\")\n",
    "forest_reg = joblib.load(MODEL_PATH+\"\\RandomForestRegressorBest.pkl\")\n",
    "\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "feature_importances = forest_reg.feature_importances_\n",
    "\n",
    "transform_and_fit = Pipeline([\n",
    "    (\"prepare\", full_pipeline),\n",
    "    ('choose_top',RemoveUnnecessaryFeatures(feature_importances,6)),\n",
    "    ('rand_reg_fit',forest_reg)\n",
    "])\n",
    "\n",
    "transform_and_fit.fit(housing,housing_labels)\n",
    "\n",
    "some_data = housing.iloc[:4]\n",
    "some_labels = housing_labels.iloc[:4]\n",
    "\n",
    "print(\"Predictions:\\t\", transform_and_fit.predict(some_data))\n",
    "print(\"Labels:\\t\\t\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d18a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
